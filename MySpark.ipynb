{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-MMJOJFK:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x205e94a3340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"Python Spark SQL basic example\")\\\n",
    ".config(\"spark.some.config.option\",\"some-value\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"housing.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('longitude', 'double'),\n",
       " ('latitude', 'double'),\n",
       " ('housing_median_age', 'double'),\n",
       " ('total_rooms', 'double'),\n",
       " ('total_bedrooms', 'double'),\n",
       " ('population', 'double'),\n",
       " ('households', 'double'),\n",
       " ('median_income', 'double'),\n",
       " ('median_house_value', 'double'),\n",
       " ('ocean_proximity', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Unique Index Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|summary|               id|          longitude|         latitude|housing_median_age|       total_rooms|    total_bedrooms|        population|       households|     median_income|median_house_value|ocean_proximity|\n",
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|  count|            20640|              20640|            20640|             20640|             20640|             20433|             20640|            20640|             20640|             20640|          20640|\n",
      "|   mean|          10319.5|-119.56970445736148| 35.6318614341087|28.639486434108527|2635.7630813953488| 537.8705525375618|1425.4767441860465|499.5396802325581|3.8706710029070246|206855.81690891474|           NULL|\n",
      "| stddev|5958.399113856003|  2.003531723502584|2.135952397457101| 12.58555761211163|2181.6152515827944|421.38507007403115|  1132.46212176534|382.3297528316098| 1.899821717945263|115395.61587441359|           NULL|\n",
      "|    min|                0|            -124.35|            32.54|               1.0|               2.0|               1.0|               3.0|              1.0|            0.4999|           14999.0|      <1H OCEAN|\n",
      "|    max|            20639|            -114.31|            41.95|              52.0|           39320.0|            6445.0|           35682.0|           6082.0|           15.0001|          500001.0|     NEAR OCEAN|\n",
      "+-------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+---------+------------------+-----------------------+\n",
      "|max(households)|median(population)|count(id)|  avg(total_rooms)|min(housing_median_age)|\n",
      "+---------------+------------------+---------+------------------+-----------------------+\n",
      "|         6082.0|            1166.0|    20640|2635.7630813953488|                    1.0|\n",
      "+---------------+------------------+---------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select( \n",
    "    'id',\n",
    "    'housing_median_age',\n",
    "    'total_rooms',    \n",
    "    'population',\n",
    "    'households'\n",
    ").agg(\n",
    "  { 'id'                :'count',\n",
    "    'housing_median_age':'min',\n",
    "    'total_rooms'       :'avg',\n",
    "    'population'        :'median',\n",
    "    'households'        :'max'\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+--------------------------+-------------------+----------------------+------------------+------------------+---------------------+--------------------------+-----------------------+\n",
      "|median(id)|median(longitude)|median(latitude)|median(housing_median_age)|median(total_rooms)|median(total_bedrooms)|median(population)|median(households)|median(median_income)|median(median_house_value)|median(ocean_proximity)|\n",
      "+----------+-----------------+----------------+--------------------------+-------------------+----------------------+------------------+------------------+---------------------+--------------------------+-----------------------+\n",
      "|   10319.5|          -118.49|           34.26|                      29.0|             2127.0|                 435.0|            1166.0|             409.0|   3.5347999999999997|                  179700.0|                   NULL|\n",
      "+----------+-----------------+----------------+--------------------------+-------------------+----------------------+------------------+------------------+---------------------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, median, stddev, count \n",
    "\n",
    "df.select(\n",
    "    *[  median(c) for c in df.columns ] \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Parameters ( Grouped by SubCats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+----------------------+---------------------+--------------------------+\n",
      "|ocean_proximity|median(households)|median(population)|median(total_bedrooms)|median(median_income)|median(median_house_value)|\n",
      "+---------------+------------------+------------------+----------------------+---------------------+--------------------------+\n",
      "|         ISLAND|             288.0|             733.0|                 512.0|               2.7361|                  414700.0|\n",
      "|     NEAR OCEAN|             429.0|            1136.5|                 464.0|              3.64705|                  229450.0|\n",
      "|       NEAR BAY|             406.0|            1033.5|                 423.0|              3.81865|                  233800.0|\n",
      "|      <1H OCEAN|             421.0|            1247.0|                 438.0|                3.875|                  214850.0|\n",
      "|         INLAND|             385.0|            1124.0|                 423.0|               2.9877|                  108500.0|\n",
      "+---------------+------------------+------------------+----------------------+---------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('ocean_proximity').agg({col: 'median' for col in df.columns[5:-1]}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[id: bigint, longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double, ocean_proximity: string],\n",
       " DataFrame[id: bigint, longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double, ocean_proximity: string])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=0)\n",
    "train, test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features_lst = df.columns \n",
    "\n",
    "numerical_features_lst.remove('id')\n",
    "numerical_features_lst.remove('ocean_proximity')\n",
    "numerical_features_lst.remove('median_house_value')\n",
    "\n",
    "numerical_features_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  3|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer( inputCols =numerical_features_lst,\n",
    "                   outputCols=numerical_features_lst )\n",
    "\n",
    "imputer = imputer.fit(train)\n",
    "\n",
    "train = imputer.transform(train)\n",
    "test  = imputer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectoring Numerical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=numerical_features_lst,\n",
    "                                             outputCol='numerical_feature_vector')\n",
    "\n",
    "train = numerical_vector_assembler.transform(train)\n",
    "test  = numerical_vector_assembler.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(numerical_feature_vector=DenseVector([-122.22, 37.86, 21.0, 7099.0, 1106.0, 2401.0, 1138.0, 8.3014])),\n",
       " Row(numerical_feature_vector=DenseVector([-122.24, 37.85, 52.0, 1467.0, 190.0, 496.0, 177.0, 7.2574]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('numerical_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Vector of Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3202293423673...|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3302106262727...|\n",
      "|  3|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|    [-122.25,37.85,52...|           [-1.3352012682254...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol =       'numerical_feature_vector',\n",
    "                        outputCol='scaled_numerical_feature_vector',\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test  = scaler.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Scaled Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(scaled_numerical_feature_vector=DenseVector([-1.3202, 1.041, -0.607, 2.0492, 1.3485, 0.8481, 1.6587, 2.3305])),\n",
       " Row(scaled_numerical_feature_vector=DenseVector([-1.3302, 1.0363, 1.856, -0.5369, -0.8279, -0.8123, -0.8392, 1.7819]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('scaled_numerical_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index_Transforming Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3202293423673...|                 3.0|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3302106262727...|                 3.0|\n",
      "|  3|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|    [-122.25,37.85,52...|           [-1.3352012682254...|                 3.0|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol ='ocean_proximity',\n",
    "                        outputCol='ocean_category_index')\n",
    "\n",
    "indexer = indexer.fit(train)\n",
    "\n",
    "train = indexer.transform(train)\n",
    "test  = indexer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Index_Transfored Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Row(ocean_category_index=0.0),\n",
       " Row(ocean_category_index=1.0),\n",
       " Row(ocean_category_index=2.0),\n",
       " Row(ocean_category_index=3.0),\n",
       " Row(ocean_category_index=4.0)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.select('ocean_category_index').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding Index_Transfored Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3202293423673...|                 3.0|         (4,[3],[1.0])|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3302106262727...|                 3.0|         (4,[3],[1.0])|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCol ='ocean_category_index',\n",
    "                                outputCol='ocean_category_one_hot')\n",
    "\n",
    "one_hot_encoder = one_hot_encoder.fit(train)\n",
    "\n",
    "train = one_hot_encoder.transform(train)\n",
    "test  = one_hot_encoder.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectoring --- *all feature columns* --- into *features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|            features|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "|  1|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|    [-122.22,37.86,21...|           [-1.3202293423673...|                 3.0|         (4,[3],[1.0])|[-1.3202293423673...|\n",
      "|  2|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|    [-122.24,37.85,52...|           [-1.3302106262727...|                 3.0|         (4,[3],[1.0])|[-1.3302106262727...|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "# vectorAssembler combines all feature columns into a single feature vector column, \"features\".\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = [ 'scaled_numerical_feature_vector',\n",
    "                  'ocean_category_one_hot'\n",
    "                ],\n",
    "    outputCol =   'features'\n",
    ")\n",
    "\n",
    "# vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "#vectorIndexer   = VectorIndexer( inputCol=\"final_feature_vector\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "train = vectorAssembler.transform(train)\n",
    "test  = vectorAssembler.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([-1.3202, 1.041, -0.607, 2.0492, 1.3485, 0.8481, 1.6587, 2.3305, 0.0, 0.0, 0.0, 1.0])),\n",
       " Row(features=DenseVector([-1.3302, 1.0363, 1.856, -0.5369, -0.8279, -0.8123, -0.8392, 1.7819, 0.0, 0.0, 0.0, 1.0]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('features').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression \n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor \n",
    "from pyspark.ml.regression import GBTRegressor, IsotonicRegression, FMRegressor\n",
    "\n",
    "from pyspark.ml.tuning     import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml            import Pipeline\n",
    "\n",
    "featuresCol = 'features'\n",
    "labelCol    = 'median_house_value' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorization Machines Regressor\n",
    "\n",
    "#fmr = FMRegressor( featuresCol=featuresCol, labelCol=labelCol )\n",
    "\n",
    "#fmr = fmr.fit(train)\n",
    "#fmr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundaries in increasing order: [-2.3832360782888644,-2.2983951650932597,-2.293404523140574,-2.28342323923521,-2.2784325972825243,-2.2584700294717894,-2.253479387519111,-2.2484887455664255,-2.2434981036137396,-2.1187320547966646,-2.113741412843986,-1.8392561054464223,-1.8342654634937365,-1.8043216117776375,-1.799330969824959,-1.7643964761561743,-1.7594058342034886,-1.7444339083454392,-1.724471340534711,-1.6595929951498343,-1.6546023531971488,-1.5647707980488512,-1.5597801560961726,-1.5198550204747094,-1.514864378522024,-1.5098737365693384,0.33167314397068987,0.3366637859233683,2.5475181709619434]\n",
      "\n",
      "Predictions associated with the boundaries: [79750.0,79750.0,83538.09523809524,83538.09523809524,90150.0,90150.0,95850.0,102260.0,107628.57142857143,107628.57142857143,108787.03703703704,108787.03703703704,111968.18181818182,111968.18181818182,114860.0,114860.0,115500.0,115500.0,153836.36363636365,153836.36363636365,184007.26363636364,184007.26363636364,188829.57391304348,188829.57391304348,189225.0,197216.5535518269,197216.5535518269,218324.40156229312,218324.40156229312]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isotonic Regression\n",
    "ir = IsotonicRegression( featuresCol=featuresCol, labelCol=labelCol )\n",
    "\n",
    "ir = ir.fit(train)\n",
    "ir\n",
    "\n",
    "print(\"Boundaries in increasing order: %s\\n\"             % str(ir.boundaries))\n",
    "print(\"Predictions associated with the boundaries: %s\\n\" % str(ir.predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTRegressionModel: uid=GBTRegressor_ca1c25305887, numTrees=20, numFeatures=12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient-Boosted Trees\n",
    "gbt = GBTRegressor( featuresCol=featuresCol, labelCol=labelCol )\n",
    "\n",
    "gbt = gbt.fit(train)\n",
    "gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "Reg = RandomForestRegressor( featuresCol=featuresCol, labelCol=labelCol )\n",
    "\n",
    "# Param Grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid( Reg.maxDepth, [4,5])\\\n",
    "  .addGrid( Reg.maxBins,  [32,64,128])\\\n",
    "  .addGrid( Reg.numTrees, [5,25,50,100])\\\n",
    "  .build()\n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=Reg.getLabelCol(),\n",
    "                                predictionCol=Reg.getPredictionCol())\n",
    "\n",
    "# CV\n",
    "cv = CrossValidator( numFolds=3, estimator=Reg, evaluator=evaluator, estimatorParamMaps=paramGrid )\n",
    "\n",
    "# Pipe Line\n",
    "rf_pipeline = Pipeline( stages=[ cv ] )\n",
    "#dt_pipeline.save('/tmp/rf_pipeline_001')\n",
    " \n",
    "# fit\n",
    "rf = rf_pipeline.fit(train) \n",
    "#rf.save('/tmp/rf_001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree \n",
    "Reg = DecisionTreeRegressor( featuresCol=featuresCol, labelCol=labelCol )\n",
    "\n",
    "# Param Grid\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid( Reg.maxDepth, [2,3,4,5])\\\n",
    "  .addGrid( Reg.maxBins,  [32, 64])\\\n",
    "  .build() \n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=Reg.getLabelCol(),\n",
    "                                predictionCol=Reg.getPredictionCol())\n",
    "\n",
    "# CV\n",
    "cv = CrossValidator( numFolds=3, estimator=Reg, evaluator=evaluator, estimatorParamMaps=paramGrid )\n",
    "\n",
    "# Pipe Line\n",
    "dt_pipeline = Pipeline( stages=[ cv ] )\n",
    "#dt_pipeline.save('/tmp/dt_pipeline_001')\n",
    " \n",
    "# fit\n",
    "dt = dt_pipeline.fit(train) \n",
    "#dt.save('/tmp/dt_001')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Linear Regression\n",
    "Reg = GeneralizedLinearRegression( featuresCol=featuresCol, labelCol=labelCol \n",
    "                                   #, family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3 \n",
    "                                 )\n",
    "\n",
    "# Param Grid\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid( Reg.family,   [\"gaussian\"])\\\n",
    "  .addGrid( Reg.link,     [\"identity\", \"log\", \"inverse\"])\\\n",
    "  .addGrid( Reg.regParam, [.25,.3,.35])\\\n",
    "  .build() \n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=Reg.getLabelCol(),\n",
    "                                predictionCol=Reg.getPredictionCol())\n",
    "\n",
    "# CV\n",
    "cv = CrossValidator( numFolds=3, estimator=Reg, evaluator=evaluator, estimatorParamMaps=paramGrid )\n",
    "\n",
    "# Pipe Line\n",
    "glr_pipeline = Pipeline( stages=[ cv ] )\n",
    "#glr_pipeline.save('/tmp/glr_pipeline_001')\n",
    " \n",
    "# fit\n",
    "glr = glr_pipeline.fit(train) \n",
    "#glr.save('/tmp/glr_001')\n",
    "\n",
    "\n",
    "''' \n",
    "Reg = GeneralizedLinearRegression( \n",
    "    featuresCol=featuresCol, labelCol=labelCol,\n",
    "    family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3 )\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"\\nCoefficients: %s\" % str(glr.coefficients))\n",
    "print(\"\\nIntercept: %s\"    % str(glr.intercept))\n",
    "print('')\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = glr.summary\n",
    "print(\"\\nCoefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"\\nT Values: \" + str(summary.tValues))\n",
    "print(\"\\nP Values: \" + str(summary.pValues))\n",
    "print(\"\\nDispersion: \" + str(summary.dispersion))\n",
    "print(\"\\nNull Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"\\nResidual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"\\nDeviance: \" + str(summary.deviance))\n",
    "print(\"\\nResidual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"\\nAIC: \" + str(summary.aic))\n",
    "print(\"\\nDeviance Residuals: \")\n",
    "summary.residuals().show(5)\n",
    "''' \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "Reg = LinearRegression( featuresCol=featuresCol, labelCol=labelCol\n",
    "                        #, maxIter=10, regParam=0.3, elasticNetParam=0.8 \n",
    "                      )\n",
    "\n",
    "# Param Grid\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid( Reg.solver,   [\"normal\",\"auto\"])\\\n",
    "  .addGrid( Reg.elasticNetParam, [.9,.8,.7])\\\n",
    "  .addGrid( Reg.regParam,     [.35,.30,.25])\\\n",
    "  .build() \n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=Reg.getLabelCol(),\n",
    "                                predictionCol=Reg.getPredictionCol())\n",
    "\n",
    "# CV\n",
    "cv = CrossValidator( numFolds=3, estimator=Reg, evaluator=evaluator, estimatorParamMaps=paramGrid )\n",
    "\n",
    "# Pipe Line\n",
    "lr_pipeline = Pipeline( stages=[ cv ] )\n",
    "#lr_pipeline.save('/tmp/lr_pipeline_001')\n",
    " \n",
    "# fit\n",
    "lr = lr_pipeline.fit(train) \n",
    "#lr.save('/tmp/lr_001')\n",
    "\n",
    "\n",
    "''' \n",
    "lr = LinearRegression( featuresCol=featuresCol, labelCol=labelCol, maxIter=10, regParam=0.3, elasticNetParam=0.8 )\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"\\nCoefficients: %s\" % str(lr.coefficients))\n",
    "print(\"\\nIntercept: %s\"    % str(lr.intercept))\n",
    "print('')\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lr.summary\n",
    "print(\"\\nnumIterations: %d\"    %     trainingSummary.totalIterations)\n",
    "print(\"\\nobjectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print('')\n",
    "trainingSummary.residuals.show(5)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\"   % trainingSummary.r2)\n",
    "''' \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_fd75a0cf06f0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = glr # lr, glr, dt, rf, gbt, ir, fmr\n",
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   \n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "| id|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|numerical_feature_vector|scaled_numerical_feature_vector|ocean_category_index|ocean_category_one_hot|            features|predicted_median_house_value|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "|  0|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|    [-122.23,37.88,41...|           [-1.3252199843200...|                 3.0|         (4,[3],[1.0])|[-1.3252199843200...|          406596.92625686596|\n",
      "|  4|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|    [-122.25,37.85,52...|           [-1.3352012682254...|                 3.0|         (4,[3],[1.0])|[-1.3352012682254...|          255582.89971551462|\n",
      "+---+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+------------------------+-------------------------------+--------------------+----------------------+--------------------+----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_df = model.transform(test).withColumnRenamed( \n",
    "    'prediction',\n",
    "    'predicted_median_house_value'\n",
    ")\n",
    "pred_test_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------+\n",
      "|median_house_value|predicted_median_house_value|\n",
      "+------------------+----------------------------+\n",
      "|          452600.0|          406596.92625686596|\n",
      "|          342200.0|          255582.89971551462|\n",
      "|          226700.0|           200815.9755520383|\n",
      "|          147500.0|          143602.71321899802|\n",
      "|          159800.0|          163263.29187966406|\n",
      "+------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_df.select( \"median_house_value\", \"predicted_median_house_value\" ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 68900.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol      = \"median_house_value\", \n",
    "    predictionCol = \"predicted_median_house_value\",\n",
    "    metricName    = \"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate( pred_test_df )\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>numerical_feature_vector</th>\n",
       "      <th>scaled_numerical_feature_vector</th>\n",
       "      <th>ocean_category_index</th>\n",
       "      <th>ocean_category_one_hot</th>\n",
       "      <th>features</th>\n",
       "      <th>predicted_median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>[-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 12...</td>\n",
       "      <td>[-1.325219984320072, 1.050342203966381, 0.9820...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>[-1.325219984320072, 1.050342203966381, 0.9820...</td>\n",
       "      <td>406596.926257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>[-122.25, 37.85, 52.0, 1627.0, 280.0, 565.0, 2...</td>\n",
       "      <td>[-1.335201268225436, 1.0362969138441056, 1.856...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>[-1.335201268225436, 1.0362969138441056, 1.856...</td>\n",
       "      <td>255582.899716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0   0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1   4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \\\n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY   \n",
       "1       565.0       259.0         3.8462            342200.0        NEAR BAY   \n",
       "\n",
       "                            numerical_feature_vector  \\\n",
       "0  [-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 12...   \n",
       "1  [-122.25, 37.85, 52.0, 1627.0, 280.0, 565.0, 2...   \n",
       "\n",
       "                     scaled_numerical_feature_vector  ocean_category_index  \\\n",
       "0  [-1.325219984320072, 1.050342203966381, 0.9820...                   3.0   \n",
       "1  [-1.335201268225436, 1.0362969138441056, 1.856...                   3.0   \n",
       "\n",
       "  ocean_category_one_hot                                           features  \\\n",
       "0   (0.0, 0.0, 0.0, 1.0)  [-1.325219984320072, 1.050342203966381, 0.9820...   \n",
       "1   (0.0, 0.0, 0.0, 1.0)  [-1.335201268225436, 1.0362969138441056, 1.856...   \n",
       "\n",
       "   predicted_median_house_value  \n",
       "0                 406596.926257  \n",
       "1                 255582.899716  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_pd_df = pred_test_df.toPandas()\n",
    "pred_test_pd_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
